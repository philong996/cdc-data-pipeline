{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a9e609",
   "metadata": {},
   "source": [
    "# Instacart Data Generation for CDC Pipeline\n",
    "\n",
    "This notebook loads Instacart data into PostgreSQL to simulate an OLTP application:\n",
    "- **Dimension tables** (aisles, departments, products): Load all data at once\n",
    "- **Transactional tables** (orders, order_products): Load incrementally to simulate real-time data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbcb3a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2656f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6cd9a",
   "metadata": {},
   "source": [
    "## 2. Database Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119ab490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'port': os.getenv('DB_PORT'),\n",
    "    'database': os.getenv('DB_NAME'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD')\n",
    "}\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = '../data'\n",
    "\n",
    "# Progress log file\n",
    "PROGRESS_LOG_FILE = '../data/loading_progress.json'\n",
    "\n",
    "# Application log file\n",
    "LOG_FILE = '../data/data_loading.log'\n",
    "\n",
    "# Batch configuration for incremental loading\n",
    "MIN_ORDERS_PER_BATCH = 50  # Minimum number of orders per batch\n",
    "MAX_ORDERS_PER_BATCH = 150  # Maximum number of orders per batch\n",
    "MIN_SLEEP_SECONDS = 1  # Minimum seconds to sleep between batches\n",
    "MAX_SLEEP_SECONDS = 10  # Maximum seconds to sleep between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfef011",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c493ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure logging to file and console\"\"\"\n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(LOG_FILE, mode='a'),\n",
    "            logging.StreamHandler()  # Also log to console for monitoring\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create and return a database connection\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error connecting to database: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_csv(filename):\n",
    "    \"\"\"Load CSV file into a pandas DataFrame\"\"\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    logger.info(f\"Loading {filename}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    logger.info(f\"Loaded {len(df)} rows from {filename}\")\n",
    "    return df\n",
    "\n",
    "def convert_to_native_types(df):\n",
    "    \"\"\"Convert numpy dtypes to native Python types\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.name.startswith('int'):\n",
    "            df[col] = df[col].astype('Int64')  # Nullable integer type\n",
    "        elif df[col].dtype.name.startswith('float'):\n",
    "            df[col] = df[col].astype('float64')\n",
    "    return df\n",
    "\n",
    "def insert_dataframe_batch(conn, df, table_name, columns):\n",
    "    \"\"\"Insert DataFrame into database table using batch insert\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert to native types\n",
    "    df = convert_to_native_types(df)\n",
    "    \n",
    "    # Prepare the insert query\n",
    "    placeholders = ','.join(['%s'] * len(columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({','.join(columns)}) VALUES ({placeholders})\"\n",
    "    \n",
    "    # Convert DataFrame to list of tuples with proper None handling\n",
    "    data = []\n",
    "    for _, row in df[columns].iterrows():\n",
    "        tuple_data = tuple(None if pd.isna(val) else int(val) if isinstance(val, (pd.Int64Dtype, int)) and not isinstance(val, bool) \n",
    "                          else float(val) if isinstance(val, float) else val \n",
    "                          for val in row)\n",
    "        data.append(tuple_data)\n",
    "    \n",
    "    # Execute batch insert\n",
    "    execute_batch(cursor, insert_query, data, page_size=1000)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    logger.info(f\"Inserted {len(data)} rows into {table_name}\")\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load progress from log file\"\"\"\n",
    "    if os.path.exists(PROGRESS_LOG_FILE):\n",
    "        with open(PROGRESS_LOG_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {'last_order_index': 0, 'total_orders_loaded': 0, 'total_order_products_loaded': 0}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Save progress to log file\"\"\"\n",
    "    os.makedirs(os.path.dirname(PROGRESS_LOG_FILE), exist_ok=True)\n",
    "    with open(PROGRESS_LOG_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "\n",
    "def insert_batch_with_products(conn, orders_batch, order_products_df):\n",
    "    \"\"\"Insert a batch of orders along with their order products\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Insert orders\n",
    "        order_columns = ['order_id', 'user_id', 'order_number', \n",
    "                        'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
    "        \n",
    "        # Convert orders batch to native types and prepare data\n",
    "        orders_batch_native = convert_to_native_types(orders_batch)\n",
    "        order_data = []\n",
    "        for _, row in orders_batch_native[order_columns].iterrows():\n",
    "            tuple_data = tuple(None if pd.isna(val) else int(val) if pd.notna(val) and isinstance(val, (int, float)) and not isinstance(val, bool)\n",
    "                              else val for val in row)\n",
    "            order_data.append(tuple_data)\n",
    "        \n",
    "        placeholders = ','.join(['%s'] * len(order_columns))\n",
    "        insert_query = f\"INSERT INTO instacart.orders ({','.join(order_columns)}) VALUES ({placeholders})\"\n",
    "        execute_batch(cursor, insert_query, order_data, page_size=len(order_data))\n",
    "        \n",
    "        # Get order products for this batch - convert order_ids to Python int\n",
    "        order_ids = [int(x) for x in orders_batch['order_id'].tolist()]\n",
    "        batch_order_products = order_products_df[order_products_df['order_id'].isin(order_ids)]\n",
    "        \n",
    "        if len(batch_order_products) > 0:\n",
    "            # Insert order products\n",
    "            order_product_columns = ['order_id', 'product_id', 'add_to_cart_order', 'reordered']\n",
    "            \n",
    "            # Convert to native types and prepare data\n",
    "            batch_order_products_native = convert_to_native_types(batch_order_products)\n",
    "            product_data = []\n",
    "            for _, row in batch_order_products_native[order_product_columns].iterrows():\n",
    "                tuple_data = tuple(int(val) if pd.notna(val) else None for val in row)\n",
    "                product_data.append(tuple_data)\n",
    "            \n",
    "            placeholders = ','.join(['%s'] * len(order_product_columns))\n",
    "            insert_query = f\"INSERT INTO instacart.order_products ({','.join(order_product_columns)}) VALUES ({placeholders})\"\n",
    "            execute_batch(cursor, insert_query, product_data, page_size=len(product_data))\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "        return len(order_data), len(batch_order_products)\n",
    "        \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        cursor.close()\n",
    "        raise e\n",
    "\n",
    "def reset_progress():\n",
    "    \"\"\"Reset the progress log to start from the beginning\"\"\"\n",
    "    if os.path.exists(PROGRESS_LOG_FILE):\n",
    "        os.remove(PROGRESS_LOG_FILE)\n",
    "        logger.info(\"Progress log reset. Next run will start from the beginning.\")\n",
    "    else:\n",
    "        logger.info(\"No progress log found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693c7b6",
   "metadata": {},
   "source": [
    "## 4. Load Dimension Data (All at Once)\n",
    "\n",
    "Dimension tables are loaded completely as they represent reference data that doesn't change frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c5373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING DIMENSION TABLES\n",
      "============================================================\n",
      "\n",
      "Loading aisles...\n",
      "Loading aisles.csv...\n",
      "Loaded 134 rows from aisles.csv\n",
      "Inserted 134 rows into instacart.aisles\n",
      "\n",
      "Loading departments...\n",
      "Loading departments.csv...\n",
      "Loaded 21 rows from departments.csv\n",
      "Inserted 21 rows into instacart.departments\n",
      "\n",
      "Loading products...\n",
      "Loading products.csv...\n",
      "Loaded 49688 rows from products.csv\n",
      "Inserted 49688 rows into instacart.products\n",
      "\n",
      "============================================================\n",
      "DIMENSION TABLES LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_dimension_tables():\n",
    "    \"\"\"Load all dimension tables (aisles, departments, products)\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"LOADING DIMENSION TABLES\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Load aisles\n",
    "        logger.info(\"Loading aisles...\")\n",
    "        aisles_df = load_csv('aisles.csv')\n",
    "        insert_dataframe_batch(conn, aisles_df, 'instacart.aisles', ['aisle_id', 'aisle'])\n",
    "        \n",
    "        # Load departments\n",
    "        logger.info(\"Loading departments...\")\n",
    "        departments_df = load_csv('departments.csv')\n",
    "        insert_dataframe_batch(conn, departments_df, 'instacart.departments', ['department_id', 'department'])\n",
    "        \n",
    "        # Load products\n",
    "        logger.info(\"Loading products...\")\n",
    "        products_df = load_csv('products.csv')\n",
    "        insert_dataframe_batch(conn, products_df, 'instacart.products', \n",
    "                              ['product_id', 'product_name', 'aisle_id', 'department_id'])\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"DIMENSION TABLES LOADED SUCCESSFULLY\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dimension tables: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Execute dimension table loading\n",
    "load_dimension_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f45d3",
   "metadata": {},
   "source": [
    "## 5. Load Train Orders Data (Incrementally)\n",
    "\n",
    "Train orders and order products are loaded incrementally to simulate real-time OLTP application behavior for CDC pipeline testing. Test data is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966886c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 22:36:32,336 - INFO - ============================================================\n",
      "2025-12-19 22:36:32,337 - INFO - LOADING TRAIN ORDERS DATA (INCREMENTALLY)\n",
      "2025-12-19 22:36:32,337 - INFO - ============================================================\n",
      "2025-12-19 22:36:32,338 - INFO - Loading orders.csv...\n",
      "2025-12-19 22:36:33,239 - INFO - Found 131209 train orders\n",
      "2025-12-19 22:36:33,239 - INFO - Loading order_products__train.csv...\n",
      "2025-12-19 22:36:33,390 - INFO - Found 1384617 train order products\n",
      "2025-12-19 22:36:33,761 - INFO - Train batch completed: 61 orders, 699 order products | Progress: 61/131209 orders (0.0%)\n",
      "2025-12-19 22:36:33,763 - INFO - Sleeping for 4 seconds...\n",
      "2025-12-19 22:36:38,171 - INFO - Train batch completed: 93 orders, 885 order products | Progress: 154/131209 orders (0.1%)\n",
      "2025-12-19 22:36:38,173 - INFO - Sleeping for 2 seconds...\n",
      "2025-12-19 22:36:40,549 - INFO - Train batch completed: 113 orders, 1104 order products | Progress: 267/131209 orders (0.2%)\n",
      "2025-12-19 22:36:40,551 - INFO - Sleeping for 3 seconds...\n",
      "2025-12-19 22:36:41,203 - WARNING - ============================================================\n",
      "2025-12-19 22:36:41,204 - WARNING - ⚠️  INTERRUPTED BY USER\n",
      "2025-12-19 22:36:41,204 - WARNING - ============================================================\n",
      "2025-12-19 22:36:41,205 - INFO - Progress saved at order index: 267\n",
      "2025-12-19 22:36:41,205 - INFO - Total orders loaded so far: 267\n",
      "2025-12-19 22:36:41,206 - INFO - Total order products loaded so far: 2688\n",
      "2025-12-19 22:36:41,206 - INFO - Remaining orders: 130942\n",
      "2025-12-19 22:36:41,207 - INFO - You can safely re-run this cell to continue from where you left off.\n",
      "2025-12-19 22:36:41,207 - WARNING - ============================================================\n"
     ]
    }
   ],
   "source": [
    "def load_train_orders_incrementally():\n",
    "    \"\"\"Load train orders and order products incrementally to simulate OLTP behavior\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(\"LOADING TRAIN ORDERS DATA (INCREMENTALLY)\")\n",
    "        logger.info(\"=\"*60)\n",
    "        \n",
    "        # Load progress\n",
    "        progress = load_progress()\n",
    "        start_index = progress['last_order_index']\n",
    "        \n",
    "        if start_index > 0:\n",
    "            logger.info(f\"Resuming from order index {start_index}\")\n",
    "            logger.info(f\"Previously loaded: {progress['total_orders_loaded']} orders, \"\n",
    "                       f\"{progress['total_order_products_loaded']} order products\")\n",
    "        \n",
    "        # Load orders\n",
    "        logger.info(\"Loading orders.csv...\")\n",
    "        orders_df = pd.read_csv(os.path.join(DATA_DIR, 'orders.csv'))\n",
    "        \n",
    "        # Filter only train orders\n",
    "        train_orders = orders_df[orders_df['eval_set'] == 'train'].sort_values('order_id').reset_index(drop=True)\n",
    "        logger.info(f\"Found {len(train_orders)} train orders\")\n",
    "        \n",
    "        # Load order products\n",
    "        logger.info(\"Loading order_products__train.csv...\")\n",
    "        train_products_df = pd.read_csv(os.path.join(DATA_DIR, 'order_products__train.csv'))\n",
    "        logger.info(f\"Found {len(train_products_df)} train order products\")\n",
    "        \n",
    "        # Process in batches with random sizes\n",
    "        current_index = start_index\n",
    "        total_orders = len(train_orders)\n",
    "        \n",
    "        try:\n",
    "            while current_index < total_orders:\n",
    "                # Random batch size\n",
    "                batch_size = random.randint(MIN_ORDERS_PER_BATCH, MAX_ORDERS_PER_BATCH)\n",
    "                end_index = min(current_index + batch_size, total_orders)\n",
    "                \n",
    "                # Get batch of orders\n",
    "                orders_batch = train_orders.iloc[current_index:end_index]\n",
    "                \n",
    "                # Insert orders and their products\n",
    "                orders_inserted, products_inserted = insert_batch_with_products(\n",
    "                    conn, orders_batch, train_products_df\n",
    "                )\n",
    "                \n",
    "                # Update progress\n",
    "                current_index = end_index\n",
    "                progress['last_order_index'] = current_index\n",
    "                progress['total_orders_loaded'] += orders_inserted\n",
    "                progress['total_order_products_loaded'] += products_inserted\n",
    "                save_progress(progress)\n",
    "                \n",
    "                # Log progress\n",
    "                logger.info(f\"Train batch completed: {orders_inserted} orders, {products_inserted} order products | \"\n",
    "                           f\"Progress: {current_index}/{total_orders} orders ({current_index/total_orders*100:.1f}%)\")\n",
    "                \n",
    "                # Sleep with random duration to simulate real-time data generation\n",
    "                if current_index < total_orders:\n",
    "                    sleep_time = random.randint(MIN_SLEEP_SECONDS, MAX_SLEEP_SECONDS)\n",
    "                    logger.info(f\"Sleeping for {sleep_time} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            logger.info(\"=\"*60)\n",
    "            logger.info(\"TRAIN ORDERS DATA LOADED SUCCESSFULLY\")\n",
    "            logger.info(f\"Total orders loaded: {progress['total_orders_loaded']}\")\n",
    "            logger.info(f\"Total order products loaded: {progress['total_order_products_loaded']}\")\n",
    "            logger.info(\"=\"*60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\"=\"*60)\n",
    "            logger.warning(\"⚠️  INTERRUPTED BY USER\")\n",
    "            logger.warning(\"=\"*60)\n",
    "            logger.info(f\"Progress saved at order index: {progress['last_order_index']}\")\n",
    "            logger.info(f\"Total orders loaded so far: {progress['total_orders_loaded']}\")\n",
    "            logger.info(f\"Total order products loaded so far: {progress['total_order_products_loaded']}\")\n",
    "            logger.info(f\"Remaining orders: {total_orders - current_index}\")\n",
    "            logger.info(\"You can safely re-run this cell to continue from where you left off.\")\n",
    "            logger.warning(\"=\"*60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading train orders: {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Execute train orders loading\n",
    "load_train_orders_incrementally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a623e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 22:35:13,976 - INFO - Progress log reset. Next run will start from the beginning.\n"
     ]
    }
   ],
   "source": [
    "# reset_progress()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
