{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5247e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/longnguyen/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/longnguyen/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6f03d9a9-1020-42e8-b3ef-d0ab5879133b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.1.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.1.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.8 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.1.0/spark-sql-kafka-0-10_2.13-4.1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;4.1.0!spark-sql-kafka-0-10_2.13.jar (89ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.1.0/spark-token-provider-kafka-0-10_2.13-4.1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.1.0!spark-token-provider-kafka-0-10_2.13.jar (247ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.2.0/scala-parallel-collections_2.13-1.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0!scala-parallel-collections_2.13.jar (117ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.1/kafka-clients-3.9.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.9.1!kafka-clients.jar (362ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.1/commons-pool2-2.12.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.12.1!commons-pool2.jar (72ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.2/hadoop-client-runtime-3.4.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.2!hadoop-client-runtime.jar (358ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.8/snappy-java-1.1.10.8.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.8!snappy-java.jar(bundle) (75ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.17/slf4j-api-2.0.17.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.17!slf4j-api.jar (59ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.2/hadoop-client-api-3.4.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.2!hadoop-client-api.jar (191ms)\n",
      ":: resolution report :: resolve 11734ms :: artifacts dl 1615ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.1.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   9   |   9   |   0   ||   11  |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6f03d9a9-1020-42e8-b3ef-d0ab5879133b\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 2 already retrieved (62237kB/158ms)\n",
      "25/12/21 08:48:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.1.0\n",
      "Master: spark://node-2:7077\n",
      "App Name: CDC-Streaming-Pipeline\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC-Streaming-Pipeline\") \\\n",
    "    .master(\"spark://node-2:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify connection\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3307bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/21 08:48:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7b5dada6-2bab-46ca-bf4c-ce72b8903170. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/12/21 08:48:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "|key                                                                                                                                                                                 |value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |topic                       |partition|offset|timestamp              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":4}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__deleted\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"__source_ts_ms\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__source_table\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Value\"},\"payload\":{\"product_id\":4,\"product_name\":\"Organic Bananas\",\"aisle_id\":3,\"department_id\":4,\"__deleted\":\"false\",\"__op\":\"r\",\"__source_ts_ms\":1766305173196,\"__source_table\":\"products\"}}  |cdc-pipeline.public.products|0        |0     |2025-12-21 08:19:34.094|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":3}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__deleted\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"__source_ts_ms\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__source_table\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Value\"},\"payload\":{\"product_id\":3,\"product_name\":\"Boneless Chicken\",\"aisle_id\":12,\"department_id\":3,\"__deleted\":\"false\",\"__op\":\"r\",\"__source_ts_ms\":1766305173196,\"__source_table\":\"products\"}}|cdc-pipeline.public.products|1        |0     |2025-12-21 08:19:34.034|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":5}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__deleted\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"__source_ts_ms\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__source_table\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Value\"},\"payload\":{\"product_id\":5,\"product_name\":\"Greek Yogurt\",\"aisle_id\":5,\"department_id\":1,\"__deleted\":\"false\",\"__op\":\"r\",\"__source_ts_ms\":1766305173196,\"__source_table\":\"products\"}}     |cdc-pipeline.public.products|2        |0     |2025-12-21 08:19:34.095|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":6}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__deleted\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"__source_ts_ms\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__source_table\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Value\"},\"payload\":{\"product_id\":6,\"product_name\":\"Cheddar Cheese\",\"aisle_id\":5,\"department_id\":1,\"__deleted\":\"false\",\"__op\":\"r\",\"__source_ts_ms\":1766305173196,\"__source_table\":\"products\"}}   |cdc-pipeline.public.products|2        |1     |2025-12-21 08:19:34.095|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":7}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__deleted\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"__source_ts_ms\"},{\"type\":\"string\",\"optional\":true,\"field\":\"__source_table\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Value\"},\"payload\":{\"product_id\":7,\"product_name\":\"test\",\"aisle_id\":1,\"department_id\":1,\"__deleted\":\"false\",\"__op\":\"c\",\"__source_ts_ms\":1766305467582,\"__source_table\":\"products\"}}             |cdc-pipeline.public.products|2        |2     |2025-12-21 08:24:29.412|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from Kafka topic using Structured Streaming\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"10.0.0.2:9092\") \\\n",
    "    .option(\"subscribe\", \"cdc-pipeline.public.products\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the Kafka message (key and value are binary)\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary data to string\n",
    "kafka_df = df.selectExpr(\n",
    "    \"CAST(key AS STRING) as key\",\n",
    "    \"CAST(value AS STRING) as value\",\n",
    "    \"topic\",\n",
    "    \"partition\",\n",
    "    \"offset\",\n",
    "    \"timestamp\"\n",
    ")\n",
    "\n",
    "# Display the streaming DataFrame\n",
    "query = kafka_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to process data\n",
    "query.awaitTermination(60)  # Run for 60 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
