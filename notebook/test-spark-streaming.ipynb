{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5247e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/longnguyen/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/longnguyen/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-adc7377b-981c-4efa-823e-e39a1dd6a6fb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.11 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;2.0.1 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.15 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.12.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;2.2.11 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.4 in central\n",
      "\tfound com.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.8.2 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.12.1 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.12.1 in central\n",
      "\tfound com.google.flogger#google-extensions;0.7.1 in central\n",
      "\tfound com.google.flogger#flogger;0.7.1 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.3 in central\n",
      "\tfound com.google.flogger#flogger-system-backend;0.7.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.11 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;2.2.11 in central\n",
      "\tfound io.grpc#grpc-api;1.50.2 in central\n",
      "\tfound io.grpc#grpc-alts;1.50.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.50.2 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.50.2 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.50.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.9.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.50.2 in central\n",
      "\tfound io.grpc#grpc-stub;1.50.2 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.50.2 in central\n",
      "\tfound io.grpc#grpc-core;1.50.2 in central\n",
      "\tfound io.grpc#grpc-census;1.50.2 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.15.0 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.5.4 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.1.7 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.8.27 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.104.5 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.8.27 in central\n",
      "\tfound com.google.api#gax;2.19.5 in central\n",
      "\tfound com.google.api#gax-grpc;2.19.5 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.10.0 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.27.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.7 in central\n",
      "\tfound io.grpc#grpc-xds;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.50.2 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.opencensus#opencensus-impl;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-impl-core;0.31.0 in central\n",
      "\tfound com.lmax#disruptor;3.4.2 in central\n",
      "\tfound io.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-exemplar-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-resource-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-exporter-metrics-util;0.31.0 in central\n",
      "\tfound com.google.cloud#google-cloud-monitoring;1.82.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-grpc-metrics;0.31.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.21 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.50.2 in central\n",
      ":: resolution report :: resolve 4041ms :: artifacts dl 168ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.104.5 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.10.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.1.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.12.1 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.12.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.8.2 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.5.4 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-monitoring;1.82.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.15.0 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.11 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n",
      "\tcom.google.flogger#flogger;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#flogger-system-backend;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#google-extensions;0.7.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.9 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.lmax#disruptor;3.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-api;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-census;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-context;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-core;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-services;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.50.2 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-exemplar-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-grpc-metrics;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-resource-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-metrics-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl-core;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.15 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.27.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.checkerframework#checker-qual;3.12.0 by [org.checkerframework#checker-qual;3.27.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.4 by [com.fasterxml.jackson.core#jackson-core;2.14.0] in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.0 by [com.google.api-client#google-api-client;2.0.1] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.2 by [com.google.http-client#google-http-client;1.42.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.2 by [com.google.http-client#google-http-client-gson;1.42.3] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.14.0 by [com.google.errorprone#error_prone_annotations;2.16] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.9.0 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\tcom.google.api#gax;2.12.2 by [com.google.api#gax;2.19.5] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.11.0 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.7.3 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\torg.threeten#threetenbp;1.5.2 by [org.threeten#threetenbp;1.6.4] in [default]\n",
      "\tcom.google.api#api-common;2.1.4 by [com.google.api#api-common;2.2.2] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.41.3 by [com.google.http-client#google-http-client;1.42.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.41.3 by [com.google.http-client#google-http-client-gson;1.42.3] in [default]\n",
      "\torg.threeten#threetenbp;1.6.0 by [org.threeten#threetenbp;1.6.4] in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.0 by [io.opencensus#opencensus-api;0.31.1] in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;1.82.0 by [com.google.cloud#google-cloud-core-grpc;2.8.27] in [default]\n",
      "\tcom.google.api#api-common;1.8.1 by [com.google.api#api-common;2.2.2] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;1.16.0 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.21 by [org.codehaus.mojo#animal-sniffer-annotations;1.22] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 by [com.google.errorprone#error_prone_annotations;2.14.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  119  |   0   |   0   |   22  ||   97  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-adc7377b-981c-4efa-823e-e39a1dd6a6fb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 97 already retrieved (0kB/66ms)\n",
      "25/12/22 16:49:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.0\n",
      "Master: spark://node-2:7077\n",
      "App Name: CDC-Streaming-Bronze-Pipeline\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC-Streaming-Bronze-Pipeline\") \\\n",
    "    .master(\"spark://node-2:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,\"\n",
    "            \"io.delta:delta-spark_2.13:4.0.0,\"\n",
    "            \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.11\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \n",
    "            \"/home/longnguyen/credentials/key.json\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify connection\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466725f7",
   "metadata": {},
   "source": [
    "# TEST: View raw Kafka messages to understand the actual structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 16:20:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1006c3cd-2f5c-42e3-b435-c722033d9b03. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/12/22 16:20:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "|key                                                                                                                                                                                 |value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |topic                       |partition|offset|timestamp              |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":4}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":4,\"product_name\":\"Organic Bananas\",\"aisle_id\":3,\"department_id\":4},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420381895,\"snapshot\":\"true\",\"db\":\"postgres\",\"sequence\":\"[null,\\\"201743256\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":886,\"lsn\":201743256,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1766420382092,\"transaction\":null}}   |cdc-pipeline.public.products|0        |0     |2025-12-22 16:19:42.447|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":3}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":3,\"product_name\":\"Boneless Chicken\",\"aisle_id\":12,\"department_id\":3},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420381895,\"snapshot\":\"first\",\"db\":\"postgres\",\"sequence\":\"[null,\\\"201743256\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":886,\"lsn\":201743256,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1766420382081,\"transaction\":null}}|cdc-pipeline.public.products|1        |0     |2025-12-22 16:19:42.419|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":7}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":7,\"product_name\":\"test\",\"aisle_id\":1,\"department_id\":1},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420158390,\"snapshot\":\"false\",\"db\":\"postgres\",\"sequence\":\"[\\\"201728400\\\",\\\"201741688\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":885,\"lsn\":201741688,\"xmin\":null},\"op\":\"c\",\"ts_ms\":1766420158460,\"transaction\":null}}    |cdc-pipeline.public.products|2        |0     |2025-12-22 16:15:58.713|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":5}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":5,\"product_name\":\"Greek Yogurt\",\"aisle_id\":5,\"department_id\":1},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420381895,\"snapshot\":\"true\",\"db\":\"postgres\",\"sequence\":\"[null,\\\"201743256\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":886,\"lsn\":201743256,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1766420382092,\"transaction\":null}}      |cdc-pipeline.public.products|2        |1     |2025-12-22 16:19:42.447|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":6}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":6,\"product_name\":\"Cheddar Cheese\",\"aisle_id\":5,\"department_id\":1},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420381895,\"snapshot\":\"true\",\"db\":\"postgres\",\"sequence\":\"[null,\\\"201743256\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":886,\"lsn\":201743256,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1766420382094,\"transaction\":null}}    |cdc-pipeline.public.products|2        |2     |2025-12-22 16:19:42.447|\n",
      "|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Key\"},\"payload\":{\"product_id\":7}}|{\"schema\":{\"type\":\"struct\",\"fields\":[{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"before\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"int32\",\"optional\":false,\"field\":\"product_id\"},{\"type\":\"string\",\"optional\":false,\"field\":\"product_name\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"aisle_id\"},{\"type\":\"int32\",\"optional\":false,\"field\":\"department_id\"}],\"optional\":true,\"name\":\"cdc-pipeline.public.products.Value\",\"field\":\"after\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"version\"},{\"type\":\"string\",\"optional\":false,\"field\":\"connector\"},{\"type\":\"string\",\"optional\":false,\"field\":\"name\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"ts_ms\"},{\"type\":\"string\",\"optional\":true,\"name\":\"io.debezium.data.Enum\",\"version\":1,\"parameters\":{\"allowed\":\"true,last,false,incremental\"},\"default\":\"false\",\"field\":\"snapshot\"},{\"type\":\"string\",\"optional\":false,\"field\":\"db\"},{\"type\":\"string\",\"optional\":true,\"field\":\"sequence\"},{\"type\":\"string\",\"optional\":false,\"field\":\"schema\"},{\"type\":\"string\",\"optional\":false,\"field\":\"table\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"txId\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"lsn\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"xmin\"}],\"optional\":false,\"name\":\"io.debezium.connector.postgresql.Source\",\"field\":\"source\"},{\"type\":\"string\",\"optional\":false,\"field\":\"op\"},{\"type\":\"int64\",\"optional\":true,\"field\":\"ts_ms\"},{\"type\":\"struct\",\"fields\":[{\"type\":\"string\",\"optional\":false,\"field\":\"id\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"total_order\"},{\"type\":\"int64\",\"optional\":false,\"field\":\"data_collection_order\"}],\"optional\":true,\"name\":\"event.block\",\"version\":1,\"field\":\"transaction\"}],\"optional\":false,\"name\":\"cdc-pipeline.public.products.Envelope\",\"version\":1},\"payload\":{\"before\":null,\"after\":{\"product_id\":7,\"product_name\":\"test\",\"aisle_id\":1,\"department_id\":1},\"source\":{\"version\":\"2.5.0.Final\",\"connector\":\"postgresql\",\"name\":\"cdc-pipeline\",\"ts_ms\":1766420381895,\"snapshot\":\"last\",\"db\":\"postgres\",\"sequence\":\"[null,\\\"201743256\\\"]\",\"schema\":\"public\",\"table\":\"products\",\"txId\":886,\"lsn\":201743256,\"xmin\":null},\"op\":\"r\",\"ts_ms\":1766420382094,\"transaction\":null}}              |cdc-pipeline.public.products|2        |3     |2025-12-22 16:19:42.447|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------+------+-----------------------+\n",
      "\n",
      "\n",
      " Raw message inspection complete. Review the 'value' column above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 16:21:02 WARN DAGScheduler: Failed to cancel job group 547ce619-8a9d-4f2f-83d6-5d4915014c46. Cannot find active jobs for it.\n",
      "25/12/22 16:21:02 WARN DAGScheduler: Failed to cancel job group 547ce619-8a9d-4f2f-83d6-5d4915014c46. Cannot find active jobs for it.\n"
     ]
    }
   ],
   "source": [
    "# TEST: View raw Kafka messages to understand the actual structure\n",
    "raw_messages = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"10.0.0.2:9092\") \\\n",
    "    .option(\"subscribe\", \"cdc-pipeline.public.products\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(key AS STRING) as key\", \"CAST(value AS STRING) as value\", \"topic\", \"partition\", \"offset\", \"timestamp\")\n",
    "\n",
    "# Display raw messages\n",
    "raw_query = raw_messages \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", \"10\") \\\n",
    "    .start()\n",
    "\n",
    "# Run for 30 seconds\n",
    "raw_query.awaitTermination(30)\n",
    "raw_query.stop()\n",
    "print(\"\\n Raw message inspection complete. Review the 'value' column above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705f5be",
   "metadata": {},
   "source": [
    "# Consume messages from Kafka topic and write to GCS Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3307bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 16:50:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Query ID: 981cfc88-8637-428a-8a81-bdf48afa12bb\n",
      "Writing to: gs://cdc-pipeline-data/uat/bronze/products\n",
      "Waiting for data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, from_json, current_timestamp, lit, to_date, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "# Define the schema for Debezium CDC payload (standard format)\n",
    "debezium_schema = StructType([\n",
    "    StructField(\"schema\", StructType([\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"fields\", StringType(), True),\n",
    "        StructField(\"optional\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"version\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"before\", StructType([\n",
    "            StructField(\"product_id\", IntegerType(), True),\n",
    "            StructField(\"product_name\", StringType(), True),\n",
    "            StructField(\"aisle_id\", IntegerType(), True),\n",
    "            StructField(\"department_id\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"after\", StructType([\n",
    "            StructField(\"product_id\", IntegerType(), True),\n",
    "            StructField(\"product_name\", StringType(), True),\n",
    "            StructField(\"aisle_id\", IntegerType(), True),\n",
    "            StructField(\"department_id\", IntegerType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"source\", StructType([\n",
    "            StructField(\"version\", StringType(), True),\n",
    "            StructField(\"connector\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"ts_ms\", LongType(), True),\n",
    "            StructField(\"snapshot\", StringType(), True),\n",
    "            StructField(\"db\", StringType(), True),\n",
    "            StructField(\"sequence\", StringType(), True),\n",
    "            StructField(\"schema\", StringType(), True),\n",
    "            StructField(\"table\", StringType(), True),\n",
    "            StructField(\"txId\", LongType(), True),\n",
    "            StructField(\"lsn\", LongType(), True),\n",
    "            StructField(\"xmin\", LongType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"op\", StringType(), True),  # Operation: c=create, u=update, d=delete, r=read\n",
    "        StructField(\"ts_ms\", LongType(), True),\n",
    "        StructField(\"transaction\", StructType([\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"total_order\", LongType(), True),\n",
    "            StructField(\"data_collection_order\", LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Read from Kafka topic using Structured Streaming\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"10.0.0.2:9092\") \\\n",
    "    .option(\"subscribe\", \"cdc-pipeline.public.products\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the Kafka message value as JSON\n",
    "parsed_df = kafka_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\", \"timestamp as kafka_timestamp\", \"offset\", \"partition\", \"topic\") \\\n",
    "    .select(\n",
    "        from_json(col(\"json_value\"), debezium_schema).alias(\"data\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"topic\")\n",
    "    )\n",
    "\n",
    "# Build bronze layer with full CDC payload and metadata\n",
    "bronze_df = parsed_df.select(\n",
    "    # CDC Payload Fields\n",
    "    col(\"data.payload.before\").alias(\"before\"),\n",
    "    col(\"data.payload.after\").alias(\"after\"),\n",
    "    col(\"data.payload.op\").alias(\"op\"),\n",
    "    col(\"data.payload.ts_ms\").alias(\"ts_ms\"),\n",
    "    \n",
    "    # Source Metadata\n",
    "    col(\"data.payload.source.db\").alias(\"source_db\"),\n",
    "    col(\"data.payload.source.table\").alias(\"source_table\"),\n",
    "    col(\"data.payload.source.ts_ms\").alias(\"source_ts_ms\"),\n",
    "    col(\"data.payload.source.txId\").cast(\"string\").alias(\"source_txn_id\"),\n",
    "    col(\"data.payload.source.lsn\").alias(\"source_lsn\"),\n",
    "    \n",
    "    # Kafka Metadata\n",
    "    col(\"topic\").alias(\"kafka_topic\"),\n",
    "    col(\"partition\").alias(\"kafka_partition\"),\n",
    "    col(\"offset\").alias(\"kafka_offset\"),\n",
    "    col(\"kafka_timestamp\").alias(\"kafka_timestamp\"),\n",
    "    \n",
    "    # Ingestion Metadata\n",
    "    current_timestamp().alias(\"ingestion_timestamp\"),\n",
    "    to_date(current_timestamp()).alias(\"ingestion_date\"),\n",
    "    when(col(\"data.payload.source.snapshot\").isin(\"true\", \"last\", \"first\"), lit(\"snapshot\"))\n",
    "        .otherwise(lit(\"incremental\")).alias(\"load_type\")\n",
    ")\n",
    "\n",
    "# GCS path for bronze layer (update with your bucket name)\n",
    "bronze_path = \"gs://cdc-pipeline-data/uat/bronze/products\"\n",
    "checkpoint_path = \"gs://cdc-pipeline-data/uat/checkpoints/products\"\n",
    "\n",
    "# Write to Delta Lake in GCS bronze layer\n",
    "query = bronze_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"path\", bronze_path) \\\n",
    "    .start()\n",
    "\n",
    "# Monitor the streaming query\n",
    "print(f\"Streaming Query ID: {query.id}\")\n",
    "print(f\"Writing to: {bronze_path}\")\n",
    "print(\"Waiting for data...\")\n",
    "\n",
    "# Keep running (adjust timeout as needed)\n",
    "query.awaitTermination(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82012e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading products from bronze layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 16:50:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 4\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- before: struct (nullable = true)\n",
      " |    |-- product_id: integer (nullable = true)\n",
      " |    |-- product_name: string (nullable = true)\n",
      " |    |-- aisle_id: integer (nullable = true)\n",
      " |    |-- department_id: integer (nullable = true)\n",
      " |-- after: struct (nullable = true)\n",
      " |    |-- product_id: integer (nullable = true)\n",
      " |    |-- product_name: string (nullable = true)\n",
      " |    |-- aisle_id: integer (nullable = true)\n",
      " |    |-- department_id: integer (nullable = true)\n",
      " |-- op: string (nullable = true)\n",
      " |-- ts_ms: long (nullable = true)\n",
      " |-- source_db: string (nullable = true)\n",
      " |-- source_table: string (nullable = true)\n",
      " |-- source_ts_ms: long (nullable = true)\n",
      " |-- source_txn_id: string (nullable = true)\n",
      " |-- source_lsn: long (nullable = true)\n",
      " |-- kafka_topic: string (nullable = true)\n",
      " |-- kafka_partition: integer (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- ingestion_timestamp: timestamp (nullable = true)\n",
      " |-- ingestion_date: date (nullable = true)\n",
      " |-- load_type: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------+---+-------------+---------+------------+-------------+-------------+----------+----------------------------+---------------+------------+-----------------------+-----------------------+--------------+---------+\n",
      "|before|after                       |op |ts_ms        |source_db|source_table|source_ts_ms |source_txn_id|source_lsn|kafka_topic                 |kafka_partition|kafka_offset|kafka_timestamp        |ingestion_timestamp    |ingestion_date|load_type|\n",
      "+------+----------------------------+---+-------------+---------+------------+-------------+-------------+----------+----------------------------+---------------+------------+-----------------------+-----------------------+--------------+---------+\n",
      "|NULL  |{3, Boneless Chicken, 12, 3}|r  |1766422158260|postgres |products    |1766422158029|888          |201744864 |cdc-pipeline.public.products|1              |0           |2025-12-22 16:49:18.691|2025-12-22 16:50:16.205|2025-12-22    |snapshot |\n",
      "|NULL  |{4, Organic Bananas, 3, 4}  |r  |1766422158272|postgres |products    |1766422158029|888          |201744864 |cdc-pipeline.public.products|0              |0           |2025-12-22 16:49:18.736|2025-12-22 16:50:16.205|2025-12-22    |snapshot |\n",
      "|NULL  |{5, Greek Yogurt, 5, 1}     |r  |1766422158273|postgres |products    |1766422158029|888          |201744864 |cdc-pipeline.public.products|2              |0           |2025-12-22 16:49:18.736|2025-12-22 16:50:16.205|2025-12-22    |snapshot |\n",
      "|NULL  |{6, Cheddar Cheese, 5, 1}   |r  |1766422158274|postgres |products    |1766422158029|888          |201744864 |cdc-pipeline.public.products|2              |1           |2025-12-22 16:49:18.742|2025-12-22 16:50:16.205|2025-12-22    |snapshot |\n",
      "+------+----------------------------+---+-------------+---------+------------+-------------+-------------+----------+----------------------------+---------------+------------+-----------------------+-----------------------+--------------+---------+\n",
      "\n",
      "\n",
      "Operation breakdown:\n",
      "+---+-----+\n",
      "| op|count|\n",
      "+---+-----+\n",
      "|  r|    4|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the product table from bronze layer\n",
    "print(\"Reading products from bronze layer...\")\n",
    "df_products = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "print(f\"Total records: {df_products.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "df_products.printSchema()\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df_products.sort(\"kafka_timestamp\").show(20, truncate=False)\n",
    "\n",
    "print(\"\\nOperation breakdown:\")\n",
    "df_products.groupBy(\"op\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640934eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleted bronze data at gs://cdc-pipeline-data/uat/bronze/products\n",
      " Deleted checkpoint at gs://cdc-pipeline-data/uat/checkpoints/products\n",
      "\n",
      " Ready to reconsume all messages from the beginning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/22 16:48:14 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:521)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:540)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:520)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:305)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:618)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:617)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:606)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:375)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "25/12/22 16:48:15 WARN KafkaOffsetReaderAdmin: Error in attempt 2 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:521)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:540)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:520)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:305)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:618)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:617)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:606)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:375)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "25/12/22 16:48:16 WARN KafkaOffsetReaderAdmin: Error in attempt 3 getting Kafka offsets: \n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:521)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:540)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:520)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:305)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:618)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:617)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:606)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:375)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "25/12/22 16:48:17 ERROR MicroBatchExecution: Query [id = 0c583a87-852f-416a-adc5-b6511cb16819, runId = f8c1ced7-dc44-4a36-91ac-cb134db4bb73] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:396)\n",
      "\tat java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:2073)\n",
      "\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions(ConsumerStrategy.scala:65)\n",
      "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.retrieveAllPartitions$(ConsumerStrategy.scala:64)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.retrieveAllPartitions(ConsumerStrategy.scala:101)\n",
      "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.assignedTopicPartitions(ConsumerStrategy.scala:112)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:521)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:540)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:520)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:305)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:618)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:617)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:606)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:602)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:375)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "Caused by: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.\n"
     ]
    }
   ],
   "source": [
    "# Delete bronze data and checkpoint to completely reset and reconsume all messages\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "# Delete bronze data\n",
    "bronze_path_obj = spark._jvm.org.apache.hadoop.fs.Path(bronze_path)\n",
    "fs_bronze = bronze_path_obj.getFileSystem(hadoop_conf)\n",
    "if fs_bronze.exists(bronze_path_obj):\n",
    "    fs_bronze.delete(bronze_path_obj, True)\n",
    "    print(f\" Deleted bronze data at {bronze_path}\")\n",
    "else:\n",
    "    print(f\"Bronze data not found at {bronze_path}\")\n",
    "\n",
    "# Delete checkpoint\n",
    "checkpoint_path_obj = spark._jvm.org.apache.hadoop.fs.Path(checkpoint_path)\n",
    "fs_checkpoint = checkpoint_path_obj.getFileSystem(hadoop_conf)\n",
    "if fs_checkpoint.exists(checkpoint_path_obj):\n",
    "    fs_checkpoint.delete(checkpoint_path_obj, True)\n",
    "    print(f\" Deleted checkpoint at {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n Ready to reconsume all messages from the beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e899f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a660f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\" Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
