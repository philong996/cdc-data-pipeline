{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe1ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077d01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdc_pipelines.common.config_loader import ConfigLoader\n",
    "from cdc_pipelines.common.spark_session import get_spark_session\n",
    "from cdc_pipelines.common.logger import setup_logging, PipelineLogger\n",
    "from cdc_pipelines.pipelines.bronze.kafka_reader import KafkaReader\n",
    "from cdc_pipelines.pipelines.bronze.bronze_writer import BronzeWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26465b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/longnguyen/cdc-data-pipeline/config/pipeline_config\"\n",
    "\n",
    "config_loader = ConfigLoader(base_dir, environment=\"dev\")\n",
    "config = config_loader.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cfde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 15:59:47,669 - __main__ - INFO - [BRONZE] Starting bronze layer pipeline\n",
      "2025-12-24 15:59:47,671 - cdc_pipelines.common.spark_session - INFO - Creating new Spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/longnguyen/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/longnguyen/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c99c2df9-3fcb-4b0b-9323-9ae8f093e2b0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.0 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.11 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;2.0.1 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.15 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.12.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;2.2.11 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.4 in central\n",
      "\tfound com.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.8.2 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.12.1 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.12.1 in central\n",
      "\tfound com.google.flogger#google-extensions;0.7.1 in central\n",
      "\tfound com.google.flogger#flogger;0.7.1 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.3 in central\n",
      "\tfound com.google.flogger#flogger-system-backend;0.7.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.11 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;2.2.11 in central\n",
      "\tfound io.grpc#grpc-api;1.50.2 in central\n",
      "\tfound io.grpc#grpc-alts;1.50.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.50.2 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.50.2 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.50.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.9.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.50.2 in central\n",
      "\tfound io.grpc#grpc-stub;1.50.2 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.50.2 in central\n",
      "\tfound io.grpc#grpc-core;1.50.2 in central\n",
      "\tfound io.grpc#grpc-census;1.50.2 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.15.0 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.5.4 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.1.7 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.8.27 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.104.5 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.8.27 in central\n",
      "\tfound com.google.api#gax;2.19.5 in central\n",
      "\tfound com.google.api#gax-grpc;2.19.5 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.10.0 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.27.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.7 in central\n",
      "\tfound io.grpc#grpc-xds;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.50.2 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.opencensus#opencensus-impl;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-impl-core;0.31.0 in central\n",
      "\tfound com.lmax#disruptor;3.4.2 in central\n",
      "\tfound io.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-exemplar-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-resource-util;0.31.0 in central\n",
      "\tfound io.opencensus#opencensus-exporter-metrics-util;0.31.0 in central\n",
      "\tfound com.google.cloud#google-cloud-monitoring;1.82.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-grpc-metrics;0.31.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.21 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.50.2 in central\n",
      ":: resolution report :: resolve 4167ms :: artifacts dl 74ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.104.5 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-monitoring-v3;1.64.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.10.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.1.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-iamcredentials;v1-rev20211203-2.0.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.12.1 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.12.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.8.2 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.5.4 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-monitoring;1.82.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.15.0 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;2.2.11 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.11 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n",
      "\tcom.google.flogger#flogger;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#flogger-system-backend;0.7.1 from central in [default]\n",
      "\tcom.google.flogger#google-extensions;0.7.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.9 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.lmax#disruptor;3.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-api;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-census;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-context;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-core;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-services;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.50.2 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-exemplar-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-grpc-metrics;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-resource-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-metrics-util;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-exporter-stats-stackdriver;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-impl-core;0.31.0 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.15 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.27.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.checkerframework#checker-qual;3.12.0 by [org.checkerframework#checker-qual;3.27.0] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.4 by [com.fasterxml.jackson.core#jackson-core;2.14.0] in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.0 by [com.google.api-client#google-api-client;2.0.1] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.2 by [com.google.http-client#google-http-client;1.42.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.2 by [com.google.http-client#google-http-client-gson;1.42.3] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.14.0 by [com.google.errorprone#error_prone_annotations;2.16] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.9.0 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\tcom.google.api#gax;2.12.2 by [com.google.api#gax;2.19.5] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.11.0 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.7.3 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\torg.threeten#threetenbp;1.5.2 by [org.threeten#threetenbp;1.6.4] in [default]\n",
      "\tcom.google.api#api-common;2.1.4 by [com.google.api#api-common;2.2.2] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.41.3 by [com.google.http-client#google-http-client;1.42.3] in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.41.3 by [com.google.http-client#google-http-client-gson;1.42.3] in [default]\n",
      "\torg.threeten#threetenbp;1.6.0 by [org.threeten#threetenbp;1.6.4] in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.0 by [io.opencensus#opencensus-api;0.31.1] in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;1.82.0 by [com.google.cloud#google-cloud-core-grpc;2.8.27] in [default]\n",
      "\tcom.google.api#api-common;1.8.1 by [com.google.api#api-common;2.2.2] in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;1.16.0 by [com.google.api.grpc#proto-google-common-protos;2.10.0] in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.21 by [org.codehaus.mojo#animal-sniffer-annotations;1.22] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 by [com.google.errorprone#error_prone_annotations;2.14.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  119  |   0   |   0   |   22  ||   97  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c99c2df9-3fcb-4b0b-9323-9ae8f093e2b0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 97 already retrieved (0kB/47ms)\n",
      "25/12/24 15:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 16:00:03,290 - cdc_pipelines.common.spark_session - INFO - Spark session created: cdc-streaming-pipeline-dev\n",
      "2025-12-24 16:00:03,294 - cdc_pipelines.common.spark_session - INFO - Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging_config = config.get(\"logging\", {})\n",
    "setup_logging(\n",
    "    log_level=logging_config.get(\"level\", \"INFO\"),\n",
    "    log_path=f\"{logging_config.get('path', 'logs')}/bronze/bronze_pipeline.log\",\n",
    ")\n",
    "\n",
    "logger = PipelineLogger(__name__, \"bronze\", \"bronze\")\n",
    "logger.info(\"Starting bronze layer pipeline\")\n",
    "\n",
    "# Create Spark session\n",
    "spark = get_spark_session(config)\n",
    "\n",
    "# Initialize components\n",
    "kafka_reader = KafkaReader(spark, config)\n",
    "\n",
    "# Get bronze configuration\n",
    "bronze_config = config.get(\"bronze\", {})\n",
    "trigger_interval = bronze_config.get(\"trigger_interval\", \"10 seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae68520",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"products\"  # for default\n",
    "\n",
    "# Determine which tables to process\n",
    "tables_to_process = {}\n",
    "if table_name:\n",
    "    # Process single table\n",
    "    table_config = config_loader.get_table_config(table_name)\n",
    "    tables_to_process[table_name] = table_config\n",
    "else:\n",
    "    # Process all tables\n",
    "    tables_to_process = config.get(\"tables\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad888518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 16:00:07,407 - __main__ - INFO - [BRONZE] Starting pipeline for table: products\n",
      "2025-12-24 16:00:07,409 - __main__ - INFO - [BRONZE] Reading from Kafka topic: cdc-pipeline.public.products\n",
      "2025-12-24 16:00:07,411 - cdc_pipelines.pipelines.bronze.kafka_reader - INFO - Reading from Kafka topics: ['cdc-pipeline.public.products']\n",
      "2025-12-24 16:00:07,411 - cdc_pipelines.pipelines.bronze.kafka_reader - INFO - Bootstrap servers: 10.0.0.2:9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 16:00:08,884 - cdc_pipelines.pipelines.bronze.kafka_reader - INFO - Successfully connected to Kafka stream\n",
      "2025-12-24 16:00:08,886 - __main__ - INFO - [BRONZE] Starting stream write for table: products\n",
      "2025-12-24 16:00:08,887 - cdc_pipelines.pipelines.bronze.bronze_writer - INFO - Writing stream to bronze table: products\n",
      "2025-12-24 16:00:08,888 - cdc_pipelines.pipelines.bronze.bronze_writer - INFO - Table path: gs://cdc-pipeline-data/uat/bronze/products\n",
      "2025-12-24 16:00:08,888 - cdc_pipelines.pipelines.bronze.bronze_writer - INFO - Checkpoint location: gs://cdc-pipeline-data/uat/bronze/checkpoints/products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/24 16:00:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 16:00:13,380 - cdc_pipelines.pipelines.bronze.bronze_writer - INFO - Stream started for table: products\n",
      "2025-12-24 16:00:13,385 - cdc_pipelines.pipelines.bronze.bronze_writer - INFO - Query ID: 8854b99a-c42c-4755-938b-fe85c89edf71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/24 16:00:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 17704 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-24 16:00:48,453 - root - ERROR - KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "2025-12-24 16:00:48,456 - py4j.java_gateway - INFO - Search for sockets that match local addr ('127.0.0.1', 45030) and remote addr ('127.0.0.1', 36557)\n",
      "2025-12-24 16:00:48,457 - py4j.java_gateway - INFO - Shutting down matched socket <socket.socket fd=84, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 45042), raddr=('127.0.0.1', 36557)>\n",
      "2025-12-24 16:00:48,458 - py4j.clientserver - INFO - Close connection stream\n",
      "2025-12-24 16:00:48,459 - py4j.clientserver - INFO - Send shutdown request for the Java socket 127.0.0.1, remote port 45030, local port 36557\n",
      "2025-12-24 16:00:48,460 - py4j.clientserver - INFO - Close connection\n",
      "2025-12-24 16:00:48,461 - py4j.clientserver - INFO - Closing down clientserver connection\n",
      "2025-12-24 16:00:48,463 - py4j.clientserver - INFO - Connection is closed\n",
      "2025-12-24 16:00:48,463 - py4j.java_gateway - INFO - Finished shutdown of socket None\n",
      "2025-12-24 16:00:48,464 - py4j.java_gateway - INFO - Shutting down the current connection <py4j.clientserver.ClientServerConnection object at 0x7fccf429ab60>\n",
      "2025-12-24 16:00:48,464 - py4j.clientserver - INFO - Close connection stream\n",
      "2025-12-24 16:00:48,465 - py4j.clientserver - INFO - Send shutdown request for the Java socket 127.0.0.1, remote port 45030, local port 36557\n",
      "2025-12-24 16:00:48,537 - py4j.clientserver - ERROR - Exception occurred while shutting down connection\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 504, in shutdown_socket\n",
      "    self.socket.sendall((\"%s\\n\" % address).encode(\"utf-8\"))\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m writer \u001b[38;5;241m=\u001b[39m BronzeWriter(config, tbl_config)\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting stream write for table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtbl_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbronze_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtbl_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter-lab/cdc-data-pipeline/notebook/../cdc_pipelines/pipelines/bronze/bronze_writer.py:78\u001b[0m, in \u001b[0;36mBronzeWriter.write_stream\u001b[0;34m(self, df, table_name, trigger_interval)\u001b[0m\n\u001b[1;32m     75\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstream_query\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Await termination (will run continuously)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[43mstream_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:225\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py:1361\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py:535\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/spark_env/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test process one table\n",
    "tbl_name, tbl_config = list(tables_to_process.items())[0]\n",
    "\n",
    "logger.info(f\"Starting pipeline for table: {tbl_name}\")\n",
    "\n",
    "# Get topic for this table\n",
    "topic = tbl_config.get(\"source_topic\")\n",
    "if not topic:\n",
    "    logger.error(f\"No source topic configured for table: {tbl_name}\")\n",
    "\n",
    "# Read from Kafka\n",
    "logger.info(f\"Reading from Kafka topic: {topic}\")\n",
    "bronze_df = kafka_reader.read_stream([topic])\n",
    "\n",
    "# Write to Delta\n",
    "writer = BronzeWriter(config, tbl_config)\n",
    "logger.info(f\"Starting stream write for table: {tbl_name}\")\n",
    "writer.write_stream(bronze_df, tbl_name, trigger_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb11df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading products from bronze layer...\n",
      "2025-12-24 16:00:56,919 - py4j.clientserver - INFO - Error while sending or receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 527, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "2025-12-24 16:00:56,920 - py4j.clientserver - INFO - Closing down clientserver connection\n",
      "2025-12-24 16:00:56,922 - root - INFO - Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 527, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/longnguyen/miniconda3/envs/spark_env/lib/python3.10/site-packages/py4j/clientserver.py\", line 530, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/24 16:00:58 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 3:=======================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 5\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- json_payload: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- ingestion_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "bronze_path = config['delta']['bronze_path']\n",
    "product_path = bronze_path + \"/products\"\n",
    "\n",
    "# Read the product table from bronze layer\n",
    "print(\"Reading products from bronze layer...\")\n",
    "df_products = spark.read.format(\"delta\").load(product_path)\n",
    "\n",
    "print(f\"Total records: {df_products.count()}\")\n",
    "print(\"\\nSchema:\")\n",
    "df_products.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
